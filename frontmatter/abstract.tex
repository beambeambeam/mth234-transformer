\chapter*{บทคัดย่อ (Abstract)}
\addcontentsline{toc}{chapter}{บทคัดย่อ (Abstract)}

รายงานฉบับนี้นำเสนอเกี่ยวกับความเชื่อมโยงระหว่างพีชคณิตเชิงเส้นและสถาปัตยกรรม Transformer ซึ่งเป็นรากฐานของแบบจำลองภาษาขนาดใหญ่ (Large Language Models: LLMs) สมัยใหม่ โดยอธิบายการทำงานขององค์ประกอบหลัก ได้แก่ การเข้ารหัสคำ การฝังตำแหน่ง กลไก Self-Attention และ Multi-Head Attention ตลอดจนโครงสร้างของ Transformer Block ผ่านมุมมองของเวกเตอร์ เมทริกซ์ การแปลงเชิงเส้น ฟังก์ชัน Softmax บรรทัดฐาน ค่าเฉพาะ และการแพร่กระจายย้อนกลับ แนวคิดทางคณิตศาสตร์เหล่านี้เป็นแกนสำคัญที่ทำให้ Transformer สามารถจับความสัมพันธ์เชิงบริบท ประมวลผลแบบขนาน และเรียนรู้โครงสร้างภาษาที่ซับซ้อนได้อย่างมีประสิทธิภาพ รายงานยังนำเสนอกรณีศึกษา เช่น Chatbot ระบบแปลภาษา และการสรุปข้อความ เพื่อแสดงการประยุกต์ใช้โมเดล Transformer ในงานจริง และส่วนสุดท้ายจบด้วยการวิเคราะห์ข้อดี ข้อจำกัด และความสำคัญของพีชคณิตเชิงเส้นต่อการพัฒนาโมเดลภาษาในยุคปัจจุบัน

\vspace{1cm}
\indent \textbf{คำสำคัญ (Keywords):} Transformer, Linear Algebra, Self-Attention, Multi-Head Attention, Large Language Models (LLMs)
