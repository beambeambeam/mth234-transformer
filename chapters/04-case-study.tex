\addtocontents{toc}{\protect\newpage}
\chapter{กรณีศึกษาหรือตัวอย่างการประยุกต์ใช้ \\ (Case Study or Example Implementation)}

\indent ส่วนนี้นำเสนอการประยุกต์ใช้โมเดลภาษาแบบ Transformer ในงานประมวลผลภาษาธรรมชาติ (Natural Language Processing: NLP) ซึ่งเป็นสาขาที่ได้รับอิทธิพลอย่างมากจากเทคโนโลยีปัญญาประดิษฐ์ โมเดลภาษาขนาดใหญ่ (Large Language Model: LLM) แสดงให้เห็นศักยภาพในการทำความเข้าใจและสร้างข้อความได้ใกล้เคียงกับมนุษย์ โดยอาศัยการเรียนรู้จากข้อมูลขนาดใหญ่และโครงสร้างพีชคณิตเชิงเส้นที่ซับซ้อน เนื้อหานี้เริ่มจากกรณีศึกษาที่พบได้จริงในอุตสาหกรรมแล้วเชื่อมโยงไปยังทฤษฎีทางคณิตศาสตร์ที่เป็นกลไกขับเคลื่อน

\section{กรณีศึกษาที่ 1: Chatbot อัจฉริยะ (Intelligent Chatbots)}

\indent Chatbot อัจฉริยะเป็นหนึ่งในตัวอย่างเด่นของการประยุกต์ใช้โมเดลภาษาแบบ Transformer ซึ่งสามารถสนทนาและตอบคำถามผู้ใช้ได้อย่างเป็นธรรมชาติและสอดคล้องกับบริบท โมเดลสมัยใหม่ เช่น ChatGPT, Google Bard หรือ Claude อาศัยสถาปัตยกรรม LLM ที่มีพื้นฐานจากเมทริกซ์เชิงเชิงเส้น

\section{กรณีศึกษาที่ 2: ระบบแปลภาษาอัตโนมัติ (Machine Translation)}

\indent ระบบแปลภาษาอัตโนมัติเป็นรูปแบบโจทย์คลาสสิกที่ทำให้สถาปัตยกรรม Transformer เป็นที่รู้จักกัน อย่างแพร่หลาย โมเดล Encoder--Decoder สามารถรับลำดับคำในภาษาต้นทางและสร้างลำดับคำในภาษา ปลายทางโดยเก็บรักษาความหมายและบริบท แม้โครงสร้างไวยากรณ์ของแต่ละภาษาจะแตกต่างกัน กลไก Self-Attention และ Cross-Attention ช่วยให้โมเดลพิจารณาความสัมพันธ์ของคำในภาษาต้นทางและเชื่อมโยงกับคำที่กำลังจะแปลในภาษาปลายทางได้อย่างแม่นยำ การดำเนินการทางพีชคณิตเชิงเส้นในเมทริกซ์น้ำหนักทำให้ โมเดลสามารถจัดการกับการเรียงลำดับคำใหม่ (Reordering) และการเลือกความหมายตามบริบท (Disambiguation) ได้อย่างมีประสิทธิภาพ

\section{กรณีศึกษาที่ 3: การสรุปความและการสร้างลำดับข้อมูล \\ (Summarization and Generation)}

\indent การสรุปใจความอัตโนมัติ (Text Summarization) และการสร้างลำดับข้อมูล (Sequence Generation) เป็นการประยุกต์ใช้ที่สะท้อนความสามารถของ LLM ในการเลือกข้อมูลสำคัญและสร้างเนื้อหาใหม่

\begin{itemize}
    \item \textbf{การสรุปความ}: โมเดลเรียนรู้ที่จะให้ความสนใจกับประโยคหรือวลีที่เป็นใจความสำคัญ โดยจะทำการคำนวณคะแนน Attention สูงให้กับส่วนที่มีน้ำหนักความหมายมาก และลดความสำคัญของรายละเอียดที่ไม่จำเป็น ผลลัพธ์คือบทสรุปที่กระชับแต่ครอบคลุม ซึ่งเป็นผลโดยตรงจากการรวมเวกเตอร์ Value ที่ได้รับค่าน้ำหนักเชิงเส้นที่เหมาะสม
    \item \textbf{การสร้างลำดับข้อมูล}: งานอย่างการเขียนโปรแกรมอัตโนมัติ การแต่งเรื่อง หรือการช่วยเขียนเอกสารอาศัยการทำนายคำถัดไป (Next Token Prediction) จากบริบทของ Prompt การคำนวณความน่าจะเป็นของโทเคนถัดไปเกิดจากการนำเวกเตอร์บริบทผ่าน Linear Layer ในชั้นสุดท้ายแล้วคำนวณ Softmax เพื่อให้ได้ distribution ของคำศัพท์ทั้งหมด
\end{itemize}


