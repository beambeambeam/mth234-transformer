\section{การวิเคราะห์ (Analysis)}

จากการศึกษาพีชคณิตเชิงเส้นในสถาปัตยกรรม Transformer สามารถวิเคราะห์และอภิปรายผลได้ดั้งนี้

\subsection{ข้อดี}

\subsubsection{การประมวลผลแบบขนาน (Parallelization)}
ความสามารถในการประมวลผลแบบขนาน ซึ่งเป็นผลโดยตรงจากการใช้กลไก Attention ที่อาศัยการคูณเมทริกซ์ (Matrix Multiplication) เป็นหลัก แตกต่างจากโมเดลแบบ Recurrent (RNN/LSTM) ที่ต้องประมวลผลโทเคนทีละตัวตามลำดับเวลา ทำให้เกิด sequential bottleneck

ใน Transformer อาศัยการคำนวณ Attention Scores ระหว่างโทเคนทั้งหมดสามารถทำได้พร้อมกันผ่านกระบวนการ Attention(Q, K, V)
\[
Q = XW_Q,\quad K = XW_K,\quad V = XW_V
\]
\[
\text{Attention}(Q,K,V) = \text{Softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
\]

\subsubsection{การหาความสัมพันธ์ระยะไกล}
ข้อจำกัดสำคัญของ RNN และ LSTM คือความยากในการเรียนรู้ความสัมพันธ์ระหว่างโทเคนที่อยู่ห่างกันมาก (Long-Range Dependencies) เนื่องจากข้อมูลต้องผ่านหลายขั้นตอน (Time Steps) ก่อนจะถึงกัน ทำให้เกิดปัญหา vanishing gradient และการสูญเสียข้อมูล

ในทางตรงกันข้าม กลไก Self-Attention ของ Transformer ช่วยให้โทเคนใดๆ สามารถเชื่อมโยงกับโทเคนอื่นๆ ในลำดับได้โดยตรง ผ่านการคำนวณผลคูณจุด (Dot Product)
ผลลัพท์จากการคำนวณผลคูณจุดนี้วัดความสัมพันธ์เชิงบริบทระหว่างโทเคนที่ตำแหน่ง i และ j โดยไม่คำนึงถึงระยะทางระหว่างกัน
\[
\alpha_{ij} = \frac{\exp\left(\frac{q_i \cdot k_j}{\sqrt{d_k}}\right)}{\sum_{\ell=1}^{n} \exp\left(\frac{q_i \cdot k_\ell}{\sqrt{d_k}}\right)}
\]

\subsection{ข้อเสีย}
\subsubsection{ความซับซ้อนเชิงกำลังสอง (Quadratic Complexity)}
ข้อจำกัดที่สำคัญที่สุดของกลไก Self-Attention คือความซับซ้อนเชิงกำลังสองของการคำนวณเมทริกซ์ $QK^\top$ ซึ่งเติบโตตาม $\mathcal{O}(n^2)$ เมื่อ $n$ คือความยาวของลำดับข้อมูล (Sequence Length) การคำนวณคะแนน Attention ระหว่างโทเคนทั้งหมดจำเป็นต้องพิจารณาคู่ของ Dot Products จำนวน $n \times n = n^2$ ชุด ส่งผลให้ทั้งเวลาและหน่วยความจำเพิ่มขึ้นอย่างรวดเร็วเมื่อความยาวอินพุตยาวขึ้น
