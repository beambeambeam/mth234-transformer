\chapter{การวิเคราะห์ (Analysis)}

\indent จากการศึกษาพีชคณิตเชิงเส้นในสถาปัตยกรรม Transformer สามารถวิเคราะห์และอภิปรายผลได้ดั้งนี้

\section{ข้อดี}

\subsection{การประมวลผลแบบขนาน (Parallelization)}
\indent ความสามารถในการประมวลผลแบบขนาน ซึ่งเป็นผลโดยตรงจากการใช้กลไก Attention ที่อาศัยการคูณ เมทริกซ์ (Matrix Multiplication) เป็นหลัก แตกต่างจากโมเดลแบบ Recurrent (RNN/LSTM) ที่ต้องประมวลผลโทเคนทีละตัวตามลำดับเวลา ทำให้เกิด sequential bottleneck
\[
Q = XW_Q,\quad K = XW_K,\quad V = XW_V
\]
\[
\text{Attention}(Q,K,V) = \text{Softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
\]

\subsection{การหาความสัมพันธ์ระยะไกล}
\indent ข้อจำกัดสำคัญของ RNN และ LSTM คือความยากในการเรียนรู้ความสัมพันธ์ระหว่างโทเคนที่อยู่ห่างกันมาก (Long-Range Dependencies) เนื่องจากข้อมูลต้องผ่านหลายขั้นตอน (Time Steps) ก่อนจะถึงกัน ทำให้เกิดปัญหา vanishing gradient และการสูญเสียข้อมูล

ในทางตรงกันข้าม กลไก Self-Attention ของ Transformer ช่วยให้โทเคนใดๆ สามารถเชื่อมโยงกับ โทเคนอื่นๆ ในลำดับได้โดยตรง ผ่านการคำนวณผลคูณจุด (Dot Product)
ผลลัพท์จากการคำนวณผลคูณจุดนี้วัดความสัมพันธ์เชิงบริบทระหว่างโทเคนที่ตำแหน่ง i และ j โดยไม่คำนึงถึงระยะทางระหว่างกัน
\[
\alpha_{ij} = \frac{\exp\left(\frac{q_i \cdot k_j}{\sqrt{d_k}}\right)}{\sum_{\ell=1}^{n} \exp\left(\frac{q_i \cdot k_\ell}{\sqrt{d_k}}\right)}
\]

\section{ข้อเสีย}
\subsection{ความซับซ้อนเชิงกำลังสอง (Quadratic Complexity)}
\indent ข้อจำกัดที่สำคัญที่สุดของกลไก Self-Attention คือความซับซ้อนเชิงกำลังสองของการคำนวณเมทริกซ์ $QK^\top$ ซึ่งเติบโตตาม $\mathcal{O}(n^2)$ เมื่อ $n$ คือความยาวของลำดับข้อมูล (Sequence Length) การคำนวณค่าคะแนน Attention ระหว่างโทเคนทั้งหมดจำเป็นต้องพิจารณาคู่ของ Dot Products จำนวน $n \times n = n^2$ ชุด ส่งผลให้ทั้งเวลาและหน่วยความจำเพิ่มขึ้นอย่างรวดเร็วเมื่อความยาวอินพุตยาวขึ้น


