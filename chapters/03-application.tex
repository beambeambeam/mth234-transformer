\chapter{บรรยายรายละเอียดในการประยุกต์ใช้ \\ (Detailed Description of the Application)}

\indent หัวข้อนี้อธิบายสถาปัตยกรรม Transformer \cite{vaswani2023attentionneed} ซึ่งเป็นแกนกลางของแบบจำลองภาษาขนาดใหญ่ (Large Language Models: LLMs) ผ่านมุมมองพีชคณิตเชิงเส้น พร้อมอธิบายเหตุผลเชิงคณิตศาสตร์ที่ทำให้โมเดลเข้าใจและสร้างภาษาได้อย่างมีประสิทธิภาพ โดยเน้นส่วนประกอบหลัก ได้แก่ การเข้ารหัสคำ การฝังตำแหน่ง กลไกความสนใจ และโครงสร้างของ Transformer Block

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{images/transformer-architecture.png}
    \caption{ภาพรวมสถาปัตยกรรม Transformer ตั้งแต่การเข้ารหัสคำไปจนถึงการเชื่อมต่อแบบ Encoder--Decoder \cite{vaswani2023attentionneed}}
    \label{fig:transformer-architecture}
\end{figure}

\newpage
\section{การเข้ารหัสคำและการฝังตำแหน่ง}\label{subsec:embedding-positional}

\indent Transformer จำเป็นต้องแปลงโทเคนเชิงสัญลักษณ์ให้เป็นเวกเตอร์เชิงตัวเลขเพื่อให้ดำเนินการเชิงเส้นได้ สององค์ประกอบที่สำคัญคือ Embedding Matrix และ Positional Encoding

\subsection{Token Embeddings}

\indent ให้ $E \in \mathbb{R}^{V \times d_{\text{model}}}$ เป็นเมทริกซ์ embedding ที่เรียนรู้ได้ ($V$ คือขนาดคลังคำศัพท์, $d_{\text{model}}$ คือมิติของเวกเตอร์) โทเคนลำดับที่ $t$ จะถูกแม็ปเป็นเวกเตอร์
\[
\mathbf{x}_t = E[t] \in \mathbb{R}^{d_{\text{model}}}
\]
เวกเตอร์เหล่านี้อาศัยโครงสร้างของปริภูมิเวกเตอร์ ทำให้การดำเนินการเชิงเส้น (เช่น การลบแปลความหมาย หรือ การคูณเมทริกซ์) สามารถจับความสัมพันธ์เชิงความหมายได้ ตัวอย่างที่พบได้บ่อย คือ $\text{king} - \text{man} + \text{woman} \approx \text{queen}$

\subsection{Positional Encoding}\label{subsubsec:pos-encoding}

\indent เนื่องจากสถาปัตยกรรม Transformer ไม่มีการวนซ้ำหรือคอนโวลูชัน โมเดลจึงต้องได้รับข้อมูลตำแหน่งเพื่อรับรู้ลำดับของโทเคน ข้อมูลนี้ถูกเพิ่มเข้าไปในเวกเตอร์ embedding แบบองค์ประกอบต่อองค์ประกอบเพื่อให้เกิดการแทนเชิงเส้นที่มีทั้งข้อมูลความหมายและลำดับ

\paragraph{Sinusoidal Positional Encoding}

\indent Positional Encoding แบบไซน์และโคไซน์นิยมใช้ เพราะเป็นฟังก์ชันเชิงกำหนด (deterministic) ที่รองรับความยาวลำดับไม่จำกัด และยังคงใช้การดำเนินการเชิงเส้นในการรวมกับ embedding สูตรมาตรฐานคือ
\[
PE_{(pos,\,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right), \qquad
PE_{(pos,\,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]
โดยที่ $pos$ คือตำแหน่งของโทเคน และ $i$ คือดัชนีมิติ คุณสมบัติหลัก ได้แก่
\begin{itemize}
    \item \textbf{ความต่อเนื่อง}: ฟังก์ชันไซน์/โคไซน์ต่อเนื่อง ทำให้โมเดลเรียนรู้ความสัมพันธ์ระยะไกลได้ดี
    \item \textbf{การทั่วไปได้ดี}: ใช้ได้กับประโยคที่ยาวกว่าข้อมูลฝึกเพราะสูตรไม่ขึ้นกับความยาวลำดับที่แน่นอน
    \item \textbf{ความสอดคล้องกับเชิงเส้น}: การรวมแบบ $\mathbf{z}_t = \mathbf{x}_t + PE_t$ ทำให้เวกเตอร์สุดท้ายยังอยู่ในปริภูมิเวกเตอร์เดียวกันและพร้อมสำหรับการคูณเมทริกซ์ในขั้นถัดไป
\end{itemize}

\newpage
\section{กลไก Self-Attention}\label{subsec:self-attention}

\indent Self-Attention เป็นแกนหลักที่ทำให้ Transformer สามารถ ``โฟกัส'' โทเคนที่เกี่ยวข้องกันภายในประโยคเดียวกันได้ ข้อมูลอินพุตถูกจัดเก็บในเมทริกซ์ $X \in \mathbb{R}^{n \times d_{\text{model}}}$ โดย $n$ คือความยาวลำดับ จากนั้นสร้างเวกเตอร์ Query, Key, Value ผ่านการฉายเชิงเส้น
\[
Q = X W_Q,\quad K = X W_K,\quad V = X W_V
\]
โดย $W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d_k}$ เป็นพารามิเตอร์ที่เรียนรู้ได้และ $d_k$ คือมิติของ Key/Query

\subsection{Scaled Dot-Product Attention}

\indent กลไกความสนใจคำนวณดังนี้
\begin{enumerate}
    \item \textbf{คะแนนความคล้ายคลึง}: $S = QK^\top \in \mathbb{R}^{n \times n}$ วัดมุมเชิงเชิงเส้นระหว่างโทเคน
    \item \textbf{การปรับสเกล}: $\tilde{S} = \frac{S}{\sqrt{d_k}}$ เพื่อลดความผันผวนเมื่อ $d_k$ ใหญ่
    \item \textbf{Softmax}: $\alpha = \text{Softmax}(\tilde{S})$ ได้การแจกแจงความน่าจะเป็นของน้ำหนักความสนใจ
    \item \textbf{รวมข้อมูล}: $\text{Attention}(Q,K,V) = \alpha V$
\end{enumerate}

ในงานทำนายโทเคนถัดไป (autoregressive) จำเป็นต้องป้องกันไม่ให้โทเคนมองเห็นข้อมูลอนาคต จึงต้องใช้เมทริกซ์หน้ากาก $M \in \mathbb{R}^{n \times n}$ ที่มีค่า $0$ เมื่อตำแหน่งอนุญาตให้มองเห็น และ $-\infty$ เมื่อไม่อนุญาต กลไกจึงปรับเป็น
\[
\text{MaskedAttention}(Q,K,V) = \text{Softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V
\]
การคำนวณทั้งหมดเป็นการคูณเมทริกซ์ จึงสามารถประมวลผลแบบขนานบน GPU/TPU ได้อย่างมีประสิทธิภาพ

\newpage
\section{กลไกแบบ Multi-Head Attention}\label{subsec:multi-head}

\indent Self-Attention เพียงหัวเดียวอาจจับได้เพียงบางแง่มุมของความสัมพันธ์ โครงสร้าง Multi-Head จึงทำซ้ำ Self-Attention หลายชุดใน subspace ที่แตกต่างกัน จากนั้นเชื่อมผลลัพธ์กลับเข้าด้วยกัน
\[
\text{head}_i = \text{Attention}(Q W_Q^{(i)}, K W_K^{(i)}, V W_V^{(i)}), \qquad
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\ldots,\text{head}_h) W_O
\]
แต่ละหัวอาจเน้นมุมมองต่างกัน เช่น โครงสร้างไวยากรณ์ ระยะทางเชิงตำแหน่ง หรือบริบทเชิงความหมาย ภาพประกอบ~\ref{fig:scaled-dot-product-multi-head} แสดงความเชื่อมโยงระหว่าง Scaled Dot-Product และ Multi-Head Attention

\begin{figure}[H]
    \centering
    \includegraphics[width=0.68\textwidth]{images/scaled-dot-product-attention-multi-head-attention.png}
    \caption{(ซ้าย) Scaled Dot-Product Attention ปรับคะแนนก่อน Softmax (ขวา) Multi-Head Attention ทำงานแบบขนานหลายหัวเพื่อดึงบริบทหลากหลาย \cite{vaswani2023attentionneed}}
    \label{fig:scaled-dot-product-multi-head}
\end{figure}

\newpage
\section{โครงสร้าง Transformer Block และ Encoder--Decoder}\label{subsec:transformer-block}

\indent แต่ละ Transformer Block ประกอบด้วยลำดับของชั้นเชิงเส้นและไม่เชิงเส้นที่ผูกกับแนวคิด Residual Connection และ Layer Normalization เพื่อรักษาเสถียรภาพการเรียนรู้

\subsection{Self-Attention Layer}

\indent อินพุต $\mathbf{x}$ จะถูกส่งผ่าน Multi-Head Self-Attention เพื่อรับเวกเตอร์บริบท $\mathbf{c}$ แล้วใช้ Residual Connection และ Layer Normalization
\[
\mathbf{y} = \text{LayerNorm}(\mathbf{x} + \text{MultiHead}(\mathbf{x}, \mathbf{x}, \mathbf{x}))
\]

\subsection{Feed-Forward Network (FFN)}

\indent แต่ละโทเคนผ่าน FFN สองชั้นที่ใช้ฟังก์ชันไม่เชิงเส้น (เช่น ReLU หรือ GELU) ทำงานแบบจุดต่อจุด
\[
\text{FFN}(x) = W_2\,\sigma(W_1 x + b_1) + b_2
\]
ผลลัพธ์ถูกห่อด้วย Residual Connection เช่นเดียวกัน: $\mathbf{z} = \text{LayerNorm}(\mathbf{y} + \text{FFN}(\mathbf{y}))$

\subsection{Encoder--Decoder Architecture}

\indent สถาปัตยกรรมดั้งเดิมของ Transformer มีสองส่วน
\begin{enumerate}
    \item \textbf{Encoder}: ซ้อนบล็อก Self-Attention+FFN หลายชั้นเพื่อสกัดบริบทของลำดับอินพุต ผลลัพธ์คือเมทริกซ์บริบทที่เข้มข้นทางความหมาย
    \item \textbf{Decoder}: ใช้ Masked Self-Attention เพื่อคาดเดาโทเคนถัดไป พร้อม Cross-Attention ที่ใช้ Query จาก Decoder และ Key/Value จาก Encoder เพื่อดึงข้อมูลต้นทางกลับมาใช้
\end{enumerate}
โมเดล LLM แบบ GPT ใช้ Decoder-only แต่ยังยึดหลักการ Masked Self-Attention, Residual Connection และ FFN เช่นเดียวกัน


